import os
import ctypes
import logging
import argparse
import itertools
import multiprocessing as mp

import numpy as np
import scipy as sp
import scipy.misc
from scipy import io as sio
from sklearn.utils import random as skrandom


def file_paths(dirname, ext):
    """Return list of file paths with given ext in dirname."""
    names = [name for name in os.listdir(dirname) if name.endswith(ext)]
    return [os.path.join(dirname, name) for name in names]


# These will be shared between multiple processes
shared_data = None
shared_indices = None
shared_indptr = None
shared_shape = None


def streaming_file_projections(
        train_dir='train', test_dir='test', file_ext='bytes', dim=256,
        percentile=75, n_jobs=-1):
    """Convert all files in given folder with given file extension to grayscale
    images and save them back to the same directory as png files.
    """
    train_paths = file_paths(train_dir, file_ext)
    test_paths = file_paths(test_dir, file_ext)
    logging.info('converting %d %s training files to png files' % (
        len(train_paths), file_ext))
    logging.info('converting %d %s testing files to png files' % (
        len(test_paths), file_ext))

    # Determine normalized file length; balance trade-off on padding vs. loss of
    # info from file trimming.
    sizes = np.array([os.path.getsize(path) for path in train_paths],
                     dtype=np.int)
    cutoff = int(dim * np.round(np.percentile(sizes, percentile) / dim))
    logging.info('using cutoff of %d' % cutoff)

    # Log some info on what kind of tradeoff is being made.
    MB = 1024 ** 3
    diff = sizes - cutoff
    trimmed = float(diff[diff > 0].sum())
    padded = float(abs(diff[diff < 0].sum()))

    logging.info('%.2fMB will be trimmed' % (trimmed / MB))
    logging.info('%.2fMB will be padded' % (padded / MB))
    logging.info('trim-to-pad ratio: %d / 1000' % (1000 * (trimmed / padded)))

    # Build random projection matrix R.
    # We interpret the cutoff as the number of features ("pixels").
    # s = 1 / density, where density = 1 / sqrt(n_features).
    n_components = dim * dim  # reduced dimension/rank after projection
    logging.info('constructing random projection matrix R (%d x %d)' % (
        cutoff, n_components))

    s = np.sqrt(cutoff)
    val = np.sqrt(s / n_components)
    vals = np.array([-1, 0, 1], dtype=np.int8)
    probs = np.array([1 / (2 * s), 1 - (1 / s), 1 / (2 * s)])
    probs = probs / probs.sum()  # remove rounding error

    # Now create R as a sparse csc matrix. This function from sklearn requires
    # "classes" for each column and class probability distributions for each.
    # https://github.com/scikit-learn/scikit-learn/blob/51a765acfa4c5d1ec05fc4b406968ad233c75162/sklearn/utils/random.py#L205
    k = vals.shape[0]
    classes = np.tile(vals, n_components).reshape(n_components, k)
    class_probs = np.tile(probs, n_components).reshape(n_components, k)
    R = skrandom.random_choice_csc(
        n_samples=cutoff, classes=classes, class_probability=class_probs)
    R = R.tocsr()  # especially suitable for fast matrix vector products

    R_nbytes = R.data.nbytes + R.indices.nbytes + R.indptr.nbytes
    R_mb = float(R_nbytes) / MB
    logging.info('done building projection matrix R (%.2fMB)' % R_mb)

    # Map work across all files, distributed based on n_jobs.
    n_jobs = n_jobs if n_jobs > 0 else (mp.cpu_count() - 2)
    logging.info('converting files using %d processes' % n_jobs)

    # Convert both train and test sets.
    all_paths = itertools.chain(train_paths, test_paths)

    if n_jobs == 1:
        arg_iter = itertools.izip(
            all_paths, itertools.repeat(cutoff), itertools.repeat(dim),
            itertools.repeat(R), itertools.repeat(file_ext))
        map(mappable_convert_and_project, arg_iter)

        save_csr_matrix(R, file_ext, percentile, dim)
        return

    # If we have more than one job, we'll want to share R.
    # Create shared memory space for the projection matrix R.
    global shared_data
    global shared_indices
    global shared_indptr
    global shared_shape

    shared_data = mp.Array(ctypes.c_double, R.data.shape[0], lock=False)
    shared_indices = mp.Array(ctypes.c_int32, R.indices.shape[0], lock=False)
    shared_indptr = mp.Array(ctypes.c_int32, R.indptr.shape[0], lock=False)
    shared_shape = mp.Array(ctypes.c_int32, len(R.shape), lock=False)

    # Fill shared memory with R data.
    shared_data[:] = R.data
    shared_indices[:] = R.indices
    shared_indptr[:] = R.indptr
    shared_shape[:] = R.shape

    arg_iter = itertools.izip(
        all_paths, itertools.repeat(cutoff), itertools.repeat(dim),
        itertools.repeat(file_ext))
    pool = mp.Pool(processes=n_jobs)
    pool.map(convert_project_shared, arg_iter)

    save_csr_matrix(R, file_ext, percentile, dim)


def save_csr_matrix(mat, ext, percentile, dim):
    """Save sparse matrix to matrix market format; filename based on args."""
    fname = 'R-%s-%d-%d.mtx' % (ext, percentile, dim)
    sio.mmwrite(fname, mat)


def convert_project_shared(arg_tuple):
    """Call convert_and_project using shared memory for projection matrix R.
    This allows sharing of R between multiple processes without pickling it for
    each call. We rely on globals for the current implementation, which does not
    work in Windows.
    """
    path, cutoff, dim, ext = arg_tuple
    R = sp.sparse.csr_matrix(
        (shared_data, shared_indices, shared_indptr),
        shape=shared_shape, copy=False)
    convert_and_project(path, cutoff, dim, R, ext)

def mappable_convert_and_project(path_cutoff_dim_R_ext):
    """Bundle args as single tuple of arguments to allow Pool.map."""
    return convert_and_project(*path_cutoff_dim_R_ext)

def convert_and_project(path, cutoff, dim, R, ext):
    """Reduce the number of pixels using random projections and convert to png.
    The file will be read from `path` and truncated or padded to be length
    `cutoff`. R will be used to project the bytes and the array will be reshaped
    to `dim` x `dim`. Finally, the file will be saved with this format:

        {original_name}_{ext}.png
    """
    logging.info('converting %s to (%d x %d) grayscale img' % (
        os.path.basename(path), dim, dim))
    arr = read_grayscale_fixed_size(path, cutoff)  # (1 x d) vector
    reduced = R.T.dot(arr[:, None])  # (k x d) x (d x 1) = (k x 1)
    img = scipy.misc.toimage(reduced.reshape(dim, dim))
    img_path = '%s_%s_%d.png' % (os.path.splitext(path)[0], ext, dim)
    img.save(img_path)


def read_grayscale_fixed_shape(filename, width=1024, height=1024):
    """Read a file as 8 bit unsigned int array with fixed size. Only width x
    height bytes will be read. If the size is less than that, then the remaining
    bytes will be padded with 0s.

    Args:
        filename (str): Path of file to read.
        width (int): Positive integer specifying final width of image.
        height (int): Positive integer specifying final height of image.
    Return:
        np.ndarray: with dimensions width x height.
    """
    logging.info('converting %s to shape %d x %d' % (
        os.path.basename(filename), width, height))

    nbytes = width * height
    arr = read_grayscale_fixed_sized(nbytes)
    return arr.reshape(width, height)


def read_grayscale_fixed_size(filename, nbytes):
    fsize = os.path.getsize(filename)
    diff = fsize - nbytes

    # Read full size of file unless nbytes specified to read is less.
    with open(filename) as f:
        if diff < 0:
            bytes_to_pad = abs(diff)
            logging.debug('padding %d bytes' % bytes_to_pad)
            arr = np.fromfile(f, count=fsize, dtype=np.uint8)
            arr = np.pad(arr, (0, bytes_to_pad), mode='constant')
        else:
            logging.debug('trimming %d bytes' % diff)
            arr = np.fromfile(f, count=nbytes, dtype=np.uint8)

    return arr


def make_parser():
    parser = argparse.ArgumentParser(
        description='parse all files in specified directory with specified ext'
                    ' into img formats')
    parser.add_argument(
        '-v', '--verbose',
        type=int, default=1,
        help='higher number = more verbose logging; 1 (INFO) by default')
    parser.add_argument(
        '-tr', '--train-dir',
        type=str, default='train',
        help='path to directory with training files')
    parser.add_argument(
        '-te', '--test-dir',
        type=str, default='test',
        help='path to directory with testing files')
    parser.add_argument(
        '-e', '--file-ext',
        type=str, choices=('bytes', 'asm'), default='bytes',
        help='type of file to parse img features from')
    parser.add_argument(
        '-p', '--percentile',
        type=int, default=75,
        help='set cutoff so that this percent of files do not lose any bytes'
             '; set to 75 by default')
    parser.add_argument(
        '-d', '--dimension',
        type=int, default=256,
        help='dimension of square image; 256 by default')
    parser.add_argument(
        '-nj', '--njobs',
        type=int, default=-1,
        help='number of jobs to use; if a number <= 0 is passed, use number of'
             ' cpus - 2; this is the default')
    return parser


if __name__ == '__main__':
    parser = make_parser()
    args = parser.parse_args()

    logging.basicConfig(
        level=(logging.INFO if args.verbose else logging.ERROR),
        format='[%(asctime)s]: %(message)s')

    streaming_file_projections(
        args.train_dir, args.test_dir, args.file_ext, args.dimension,
        args.percentile, args.njobs)

