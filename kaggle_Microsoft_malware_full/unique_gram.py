import os
import gc
import sys
import logging
import argparse
import functools
import itertools
from csv import DictReader
from collections import Counter
import multiprocessing as mp

import pandas as pd


def window(iterable, n):
    """Iterate over iterable by sliding window of width n across."""
    iters = itertools.tee(iterable, n)
    for i in xrange(1, n):
        for each in iters[i:]:
            next(each, None)
    return itertools.izip(*iters)


def load_labels(path, label):
    """Load the ids for all records with the given label from the training
    labels file.

    Args:
        path (str): Path to file with training labels.
        label (int): An int between 1 and 9, inclusive.
    Return:
        A list of strings, where each string is an ID associated with a sample
        with class `label`.
    """
    with open(path) as f:
        return [row['Id'] for row in DictReader(f)
                if int(row['Class']) == label]


def unique_iter_counter(iterable):
    """Return a Counter with unique elements as keys and 1 for each value."""
    elements = Counter()
    for el in iterable:
        elements[el] = 1
    return elements


def parse_unique_ngrams(fname, n=4):
    """Generate grams dictionary for one bytes file.

    Args:
        fname (str): Basename of bytes file to parse n-grams from.
        n (int): Length of n-grams to parse.
    Returns:
        dict: n-grams as keys and 1 for each value.
    """
    path = "train/%s.bytes" % fname
    with open(path, 'rb') as f:
        token_lists = (line.rstrip().split(" ")[1:] for line in f)
        tokens = itertools.chain.from_iterable(token_lists)

        # Parse out n-grams by sliding a window over the list of tokens.
        grams_string = (''.join(tup) for tup in window(tokens, n))

        # Return Counter dict with 1s for all unique n-grams.
        return unique_iter_counter(grams_string)


def mappable_parse_unique_ngrams(fname_and_n):
    """Take a list of args and pass through as separate args."""
    return parse_unique_ngrams(*fname_and_n)


def aggregate_unique_counts(fnames, n):
    """Add up n-gram dictionaries."""
    pool = mp.Pool(mp.cpu_count() - 2)
    counts = Counter()
    for i, file_counts in enumerate(pool.imap_unordered(
            mappable_parse_unique_ngrams,
            itertools.izip(fnames, itertools.repeat(n)))):
        logging.info('parsed %d files' % i)
        counts += file_counts
        logging.info('vocabulary size: %d' % len(counts))
    return counts


def make_parser():
    parser = argparse.ArgumentParser(
        description='parse unique ngram counts from bytes files')
    parser.add_argument(
        '-v', '--verbose',
        type=int, default=1,
        help='enable verbose logging output if set to 1; enabled by default')
    parser.add_argument(
        '-n',
        type=int, default=4,
        help='length of n-grams to parse')
    parser.add_argument(
        '-tn', '--top-n',
        type=int, default=100000,
        help='top n most common words to keep after counting; ties are'
             ' resolved arbitrarily')
    parser.add_argument(
        '-lf', '--label-file',
        type=str, default='trainLabels.csv',
        help='path to training data label file')
    parser.add_argument(
        '-s', '--savedir',
        type=str, default='ngrams',
        help='path of directory in which to save results')
    return parser


if __name__ == '__main__':
    parser = make_parser()
    args = parser.parse_args()

    logging.basicConfig(
        level=(logging.INFO if args.verbose else logging.ERROR),
        format='[%(asctime)s]: %(message)s')

    n_classes = 9
    for label in range(1, n_classes + 1):
        logging.info('gathering %d-grams, class %d out of %d...' % (
            args.n, label, n_classes))

        # Get all unique n-grams from each bytes file for the given label,
        # then aggregate the counts together and find the top_n most common.
        byte_fnames = load_labels(args.label_file, label)
        ngram_counts = aggregate_unique_counts(byte_fnames, args.n)
        most_common = ngram_counts.most_common(args.top_n)

        # Save the results.
        df = pd.DataFrame(most_common)
        save_fname = '%d-grams-class%d-top%d' % (
            args.n, label, args.top_n)
        save_path = os.path.join(args.savedir, save_fname)
        df.to_csv(save_path, header=False, index=False)

